{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3acee88b-79dd-4269-b4a6-979b97b3dfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 970 validated image filenames belonging to 4 classes.\n",
      "Found 244 validated image filenames belonging to 4 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sagnikdey/Downloads/FINAL/VENV/lib/python3.10/site-packages/keras/src/legacy/preprocessing/image.py:920: UserWarning: Found 7 invalid image filename(s) in x_col=\"image_id\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n",
      "/Users/sagnikdey/Downloads/FINAL/VENV/lib/python3.10/site-packages/keras/src/legacy/preprocessing/image.py:920: UserWarning: Found 1 invalid image filename(s) in x_col=\"image_id\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n",
      "/Users/sagnikdey/Downloads/FINAL/VENV/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "/Users/sagnikdey/Downloads/FINAL/VENV/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 533ms/step - accuracy: 0.6096 - loss: 1.8938 - val_accuracy: 0.5984 - val_loss: 3.7331 - learning_rate: 0.0010\n",
      "Epoch 2/25\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 525ms/step - accuracy: 0.7999 - loss: 0.7037 - val_accuracy: 0.3648 - val_loss: 3.3523 - learning_rate: 0.0010\n",
      "Epoch 3/25\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 503ms/step - accuracy: 0.8194 - loss: 0.6180 - val_accuracy: 0.6885 - val_loss: 2.3079 - learning_rate: 0.0010\n",
      "Epoch 4/25\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 512ms/step - accuracy: 0.8032 - loss: 0.6941 - val_accuracy: 0.6762 - val_loss: 1.0525 - learning_rate: 0.0010\n",
      "Epoch 5/25\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 480ms/step - accuracy: 0.8474 - loss: 0.5497\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 510ms/step - accuracy: 0.8467 - loss: 0.5504 - val_accuracy: 0.4549 - val_loss: 1.5856 - learning_rate: 0.0010\n",
      "Epoch 6/25\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 480ms/step - accuracy: 0.8627 - loss: 0.5192\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 511ms/step - accuracy: 0.8625 - loss: 0.5196 - val_accuracy: 0.4467 - val_loss: 1.4041 - learning_rate: 5.0000e-04\n",
      "Epoch 7/25\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 480ms/step - accuracy: 0.8710 - loss: 0.4740\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 510ms/step - accuracy: 0.8714 - loss: 0.4738 - val_accuracy: 0.4836 - val_loss: 1.3144 - learning_rate: 2.5000e-04\n",
      "Epoch 8/25\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 471ms/step - accuracy: 0.8779 - loss: 0.5049\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 503ms/step - accuracy: 0.8780 - loss: 0.5041 - val_accuracy: 0.4713 - val_loss: 1.3025 - learning_rate: 1.2500e-04\n",
      "Epoch 9/25\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 465ms/step - accuracy: 0.8683 - loss: 0.5051\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 496ms/step - accuracy: 0.8686 - loss: 0.5040 - val_accuracy: 0.4877 - val_loss: 1.2554 - learning_rate: 6.2500e-05\n",
      "Epoch 10/25\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 471ms/step - accuracy: 0.8888 - loss: 0.4458\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 500ms/step - accuracy: 0.8888 - loss: 0.4459 - val_accuracy: 0.5410 - val_loss: 1.1422 - learning_rate: 3.1250e-05\n",
      "Epoch 11/25\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 517ms/step - accuracy: 0.8944 - loss: 0.4603 - val_accuracy: 0.6352 - val_loss: 0.9841 - learning_rate: 1.5625e-05\n",
      "Epoch 12/25\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 496ms/step - accuracy: 0.9141 - loss: 0.3974 - val_accuracy: 0.7377 - val_loss: 0.8199 - learning_rate: 1.5625e-05\n",
      "Epoch 13/25\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 511ms/step - accuracy: 0.9069 - loss: 0.4222 - val_accuracy: 0.7787 - val_loss: 0.6881 - learning_rate: 1.5625e-05\n",
      "Epoch 14/25\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 511ms/step - accuracy: 0.8892 - loss: 0.4606 - val_accuracy: 0.8279 - val_loss: 0.6370 - learning_rate: 1.5625e-05\n",
      "Epoch 15/25\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467ms/step - accuracy: 0.8692 - loss: 0.4734\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 498ms/step - accuracy: 0.8699 - loss: 0.4724 - val_accuracy: 0.8361 - val_loss: 0.6374 - learning_rate: 1.5625e-05\n",
      "Epoch 16/25\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 468ms/step - accuracy: 0.8797 - loss: 0.4504\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 497ms/step - accuracy: 0.8801 - loss: 0.4499 - val_accuracy: 0.8566 - val_loss: 0.6532 - learning_rate: 7.8125e-06\n",
      "Epoch 17/25\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 468ms/step - accuracy: 0.8956 - loss: 0.4463\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 497ms/step - accuracy: 0.8956 - loss: 0.4464 - val_accuracy: 0.8607 - val_loss: 0.6890 - learning_rate: 3.9063e-06\n",
      "Epoch 18/25\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 491ms/step - accuracy: 0.8715 - loss: 0.4902\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 524ms/step - accuracy: 0.8722 - loss: 0.4893 - val_accuracy: 0.8607 - val_loss: 0.7126 - learning_rate: 1.9531e-06\n",
      "Epoch 19/25\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 482ms/step - accuracy: 0.8851 - loss: 0.4623\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 514ms/step - accuracy: 0.8851 - loss: 0.4626 - val_accuracy: 0.8525 - val_loss: 0.7389 - learning_rate: 9.7656e-07\n",
      "Epoch 20/25\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 476ms/step - accuracy: 0.8752 - loss: 0.4999\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 507ms/step - accuracy: 0.8752 - loss: 0.4995 - val_accuracy: 0.8525 - val_loss: 0.7719 - learning_rate: 4.8828e-07\n",
      "Epoch 21/25\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 481ms/step - accuracy: 0.8900 - loss: 0.4718\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 510ms/step - accuracy: 0.8900 - loss: 0.4711 - val_accuracy: 0.8730 - val_loss: 0.8196 - learning_rate: 2.4414e-07\n",
      "Epoch 22/25\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 480ms/step - accuracy: 0.8859 - loss: 0.4847\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 511ms/step - accuracy: 0.8863 - loss: 0.4831 - val_accuracy: 0.8730 - val_loss: 0.8711 - learning_rate: 1.2207e-07\n",
      "Epoch 23/25\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 463ms/step - accuracy: 0.9108 - loss: 0.4372\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 493ms/step - accuracy: 0.9104 - loss: 0.4379 - val_accuracy: 0.8648 - val_loss: 0.9214 - learning_rate: 6.1035e-08\n",
      "Epoch 24/25\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 475ms/step - accuracy: 0.8872 - loss: 0.4696\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 503ms/step - accuracy: 0.8875 - loss: 0.4693 - val_accuracy: 0.8648 - val_loss: 0.9739 - learning_rate: 3.0518e-08\n",
      "Epoch 25/25\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467ms/step - accuracy: 0.8770 - loss: 0.4611\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 498ms/step - accuracy: 0.8776 - loss: 0.4604 - val_accuracy: 0.8689 - val_loss: 1.0335 - learning_rate: 1.5259e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved model trained and saved at: /Users/sagnikdey/Downloads/FINAL/data/soil_classifier_model.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ensure src directory is in the path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../src\")))\n",
    "from preprocessing import (\n",
    "    load_train_data,\n",
    "    encode_labels,\n",
    "    compute_class_weights,\n",
    "    save_label_encoder,\n",
    ")\n",
    "\n",
    "# ✅ PATH SETUP\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"../data\"))\n",
    "TRAIN_FOLDER = os.path.join(BASE_DIR, \"train\")\n",
    "TRAIN_CSV = os.path.join(BASE_DIR, \"train_labels.csv\")\n",
    "LABEL_ENCODER_PATH = os.path.join(BASE_DIR, \"label_encoder_classes.json\")\n",
    "MODEL_PATH = os.path.join(BASE_DIR, \"soil_classifier_model.h5\")\n",
    "\n",
    "# ✅ LOAD AND PREPROCESS DATA\n",
    "train_df = load_train_data(TRAIN_CSV, TRAIN_FOLDER)\n",
    "train_df, le = encode_labels(train_df)\n",
    "save_label_encoder(le, LABEL_ENCODER_PATH)\n",
    "\n",
    "# ✅ SPLIT TRAIN/VAL\n",
    "train_data, val_data = train_test_split(\n",
    "    train_df, test_size=0.2, stratify=train_df[\"label\"], random_state=42\n",
    ")\n",
    "class_weights = compute_class_weights(train_data[\"label\"])\n",
    "\n",
    "# ✅ IMAGE GENERATORS\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "train_gen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=30,\n",
    "    zoom_range=0.3,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    ")\n",
    "val_gen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "train_flow = train_gen.flow_from_dataframe(\n",
    "    train_data,\n",
    "    TRAIN_FOLDER,\n",
    "    x_col=\"image_id\",\n",
    "    y_col=\"soil_type\",\n",
    "    target_size=IMG_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "val_flow = val_gen.flow_from_dataframe(\n",
    "    val_data,\n",
    "    TRAIN_FOLDER,\n",
    "    x_col=\"image_id\",\n",
    "    y_col=\"soil_type\",\n",
    "    target_size=IMG_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# ✅ IMPROVED MODEL DEFINITION\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Conv2D(\n",
    "            32, (3, 3), padding=\"same\", activation=\"relu\", input_shape=(224, 224, 3)\n",
    "        ),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\"),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\"),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        tf.keras.layers.Dense(4, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.05),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "\n",
    "# ✅ CALLBACKS\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", patience=1, factor=0.5, verbose=1),\n",
    "    # EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
    "]\n",
    "\n",
    "# ✅ TRAIN\n",
    "history = model.fit(\n",
    "    train_flow,\n",
    "    validation_data=val_flow,\n",
    "    epochs=25,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights,\n",
    ")\n",
    "\n",
    "# ✅ SAVE MODEL\n",
    "model.save(MODEL_PATH)\n",
    "print(f\"Improved model trained and saved at: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffddb92b-1fe5-4338-8966-cca01c8a53c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
